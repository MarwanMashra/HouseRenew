{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a1bc5f",
   "metadata": {},
   "source": [
    "GROUP NAME  \n",
    "\n",
    "HI!CKATHON 2023\n",
    "\n",
    "Date  \n",
    "\n",
    "# I.\tOVERVIEW\n",
    "## 1.\tProject Background and Description\n",
    " \tDescribe how this project came about, and the purpose.\n",
    "Nowadays the concern for energy saving is a major issue not only in Europe but all over the world, that's why major efforts to reduce collective and individual consumption of energy haven been implemented in industries such as transportation, hotel industry, heating at home,etc however, the persistent element that all of them share is building location. To do so we could think of features like location, size of building or main usage. A data set continaing 71 explicative variables was given to us to find the answer to what contributes to energy sobriety in buildings, but above all to find a simple way to decrease the excess of energy. \n",
    "\n",
    "\n",
    "## 2.\tProject Scope\n",
    " \t Scope answers questions including what will be done, what won’t be done, and what the result will look like.\n",
    "\n",
    "Find the main characteristics that contribute to building energy consumption what are the measures we can implement in order to reduce it. We will not explain why there are differences during the pass of the time or \n",
    "\n",
    "## 3.\tPresentation of the group\n",
    " \tInclude your specialization at school, etc.\n",
    "\n",
    "| First name | Last name | Year of studies & Profile | School | Skills | Roles/Tasks | Observations |\n",
    "| ---------- | --------- | ------------------------- | ------ | ------ | ----------- | ------------ |\n",
    "| Luc    | Gensbittel  |M2 Data sciences  | IP Paris | Management, data sciences | idem     | Do always as to be proud of what you do |\n",
    "| Marisol    | Jiménez  |M2 MSID  | UPPA | Data extraction, statistiques | Data scientist, integration of tasks    |  |\n",
    "\n",
    "\n",
    "\n",
    "## 4.\tTask Management\n",
    " \tDescribe how you interacted and collaborated as a team, and the effect of every member’s unique background on the project.\n",
    "We got to know the experience of each member and decided to split the charges into two main groups with business and coding approches. The bussiness approach find a solution for energy saving while scoping our final client meanwhile inside the coding group they developped the tasks such as visualization, exploratory data analysis and of course, model developing.    \n",
    "\n",
    "\n",
    "\n",
    "# II.\tPROJECT MANAGEMENT\n",
    "## 1.\tData Understanding\n",
    " \tProvided the initial collection of data has already occurred, \n",
    "    this step includes identifying and defining the relevant data, exploring the range, scale, formats, contents, and biases of the data, and evaluating the quality and validity of the resulting data.\n",
    "We started by exploring the correlation matrix to find the contribution of each feature to the model and focus our efforts on a better cleaning of the relavant data. We detected the number of NAS per feature to decide how to handle those values, taking on acount the contribution of the feature to the model, for example, the feature building_year had almost 7% of missing values while it contributed largely to the model, however the variable building_period had a 0% of missing values, given the fact that they were mostly correlated we wondered if we could complete one with the help of the other. We identified mainly two types of data: numeric and categorical to procede with a separate treatment for each. Regarding the biases, we plotted the numerical values and found relatevily few outliers meaning we could get rid of them without a great loss of information.\n",
    "\n",
    "## 2.\tData Pre-processing\n",
    " \tExplain how the selection of data was manipulated and modified to remove redundant features and improve the quality of the data. Describe the preprocessing techniques used, such as data augmentation.\n",
    "Instead of just deleting the rows containing NAs we tried to complete the missing values with the help of similar variables. Retaking the case of NAs in the feature building_year, we completed them with the mean by each building_period, however we found that for the categories 2000-2005 and 2006-2012 the mean was under 2000 which made no sense, so in those two categories we took instead the mode which gave us 2005, and 2008, respectively. To remove redundant and irrelevant features we relied on the percentage of NAs, for example the feature gas_meters_total had almost 99% of NAs which meant was adding no information at all! We deleted the features on a similar situation. \n",
    "\n",
    "Understan what NAs actually mean in the feature, is it really a missing value? or a category that does not belong to any of the defined ones, A Na means there is no heating device different to say we do not know,  so we attribute a new category to this type of data. \n",
    "balcony depth is nan, but at the end it means there is no balcon, so its measure is actually zero, bearing wall material ya existe una columna q se llama other entonces we could assing to this category all the nas. Apply mean directly for nan on numeric columns. Finally we delete the remaining rows with nan values and we keep just approximately 600,000 rows. Mapping of categorical variables avec numerical encoding, with balcony depth.\n",
    "\n",
    "pour enlevr les outliers on a mit les limits based on watching the max and min from the graphs for target and height of biulding wich seem a big difference\n",
    "condo, individual, we write 2 columns and see if they are present feature engineering\n",
    "\n",
    "pour material we added as much columns as materials available and we turned it into bynary variables. one hot incoder\n",
    "\n",
    "\n",
    "\n",
    "## 3.\tModeling Development\n",
    " \tDescribe how you selected algorithms, how you calibrated them according to the data and how - in fine - you selected the best AI model using a well-defined set of metrics.\n",
    "\n",
    "We focused on a quick adjustment of the model to see a general behavior of our data training, get a variance to improve and above of all identify globally the important feautres of wich the business team could start to develop our During our first trial implemented the model of XGBoosting because it is the most used for data with random forests and parametres par default, we got a explained variance of 81%, after changin the parametres and seeing no big changes on hte predictions we returned to the cleaning of the data to improve the model.\n",
    "\n",
    "## 4.\t Deployment Strategy\n",
    " \tWhat best practices/norms did you follow? How do you plan on deploying your IA solution?\n",
    "Enter your answer here.\n",
    "\n",
    "\n",
    "# III.\tCARBON FOOTPRINT LIMITATION\n",
    " \tDescribe the taken measures/actions during the development of your solution in view of limiting the carbon footprint.\n",
    "By keeeping only relevant columns we do not only assure a more accurate reponse but also levering\n",
    "\n",
    "\n",
    "# IV.\tCONCLUSION\n",
    " \tTell us about the actual results, their limitations as well as future perspectives and improvements.\n",
    "Enter your answer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05f0344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nbconvert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
